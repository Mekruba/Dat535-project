{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT535 Project\n",
    "---\n",
    "\n",
    "## Magnus Egelland & Ninh Bao Truong\n",
    "\n",
    "This notebook serves as the main file for the DAT535 project. It processes data from `anime-dataset-2023.csv`, which contains comprehensive information about anime from MyAnimeList (MAL).\n",
    "\n",
    "The notebook extracts and filters the data, preparing it for training a simple linear regression model. The model predicts anime scores based on features such as genre and studio, among others. Given the dataset size of approximately 15 MB, it was also used light-load benchmark for processing and analysis, while calulating average animme score from user score is the heavy load bemchmark(`average_anime_score.ipynb`).\n",
    "\n",
    "\n",
    "### Light Load Benchmark: Anime Dataset Cleaning and Score prediction\n",
    "\n",
    "This notebook focuses on performing a light load benchmark to evaluate the efficiency of PySpark in handling smaller datasets with moderate computational requirements. The primary goal is to clean, preprocess, and analyze an anime dataset to extract meaningful insights and prepare it for machine learning workflows.\n",
    "\n",
    "The workflow is divided into two key stages:\n",
    "\n",
    "1. **Data Cleaning and Preprocessing**:\n",
    "   - The raw anime dataset is cleaned to handle multiline records, escape special characters, and normalize field counts.\n",
    "   - Selected columns are extracted, and the dataset is saved in a structured format for further analysis.\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - The cleaned dataset is processed to compute average scores for genres and studios, transforming string-based fields into numerical representations.\n",
    "   - These features are structured into labeled datasets, making them suitable for predictive modeling.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset # reset all stored variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing Section\n",
    "\n",
    "This section focuses on cleaning and preprocessing a raw anime dataset for further analysis. The key steps performed are as follows:\n",
    "\n",
    "1. **Spark Session Configuration**: \n",
    "   A Spark session is initialized with specific configurations for memory, cores, and instances to handle distributed data processing efficiently.\n",
    "\n",
    "2. **Data Loading**: \n",
    "   The dataset is loaded as an RDD (Resilient Distributed Dataset) from a CSV file. The header row is identified and excluded from the main dataset.\n",
    "\n",
    "3. **Multiline Record Cleaning**: \n",
    "   Multiline records in the dataset are identified and combined into single records using a cleaning function. This ensures that each record corresponds to a single line, preventing parsing issues.\n",
    "\n",
    "4. **Quote Handling and Parsing**: \n",
    "   A custom function is used to handle embedded quotes and escape characters within fields. This ensures proper parsing of the dataset while maintaining the integrity of quoted fields.\n",
    "\n",
    "5. **Field Normalization**: \n",
    "   Each record is normalized to ensure it contains exactly 24 fields:\n",
    "   - Missing fields are padded with `None`.\n",
    "   - Excess fields are consolidated into the last field.\n",
    "\n",
    "6. **RDD to DataFrame Conversion**: \n",
    "   The cleaned and parsed RDD is converted into a Spark DataFrame, using the original header as column names.\n",
    "\n",
    "7. **DataFrame Preview and Storage**: \n",
    "   The resulting DataFrame is previewed to verify the cleaning process and saved as a cleaned CSV file for downstream tasks. The file is written with headers, and internal quotes are properly escaped.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import csv\n",
    "from io import StringIO\n",
    "import re\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from io import StringIO\n",
    "import csv\n",
    "# Create a Spark session\n",
    "\n",
    "# Variables for Spark configuration\n",
    "spark_master = \"yarn\"\n",
    "driver_memory = \"2048m\"\n",
    "am_memory = \"1024m\"\n",
    "executor_memory = \"1g\"\n",
    "executor_cores = \"4\"\n",
    "executor_instances = \"3\"\n",
    "max_cores = \"12\"\n",
    "\n",
    "# Create a Spark session with the configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Benchmark light load 3 config 5\") \\\n",
    "    .master(spark_master) \\\n",
    "    .config(\"spark.driver.memory\", driver_memory) \\\n",
    "    .config(\"spark.yarn.am.memory\", am_memory) \\\n",
    "    .config(\"spark.executor.memory\", executor_memory) \\\n",
    "    .config(\"spark.executor.cores\", executor_cores) \\\n",
    "    .config(\"spark.executor.instances\", executor_instances) \\\n",
    "    .config(\"spark.cores.max\", max_cores) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Define the file path\n",
    "filepath = \"project/anime-dataset-2023_copy.csv\"\n",
    "\n",
    "# Read the CSV file as an RDD of strings\n",
    "rdd = spark.sparkContext.textFile(filepath)\n",
    "\n",
    "# Extract the header\n",
    "header = rdd.first()\n",
    "data_rdd = rdd.filter(lambda line: line != header)  # Filter out the header row\n",
    "\n",
    "# Function to clean and parse multiline records\n",
    "def clean_and_parse(lines):\n",
    "    cleaned_lines = []\n",
    "    current_line = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        # Check if the line starts with a digit (indicating a new record)\n",
    "        if re.match(r'^\\d+,.+', line):\n",
    "            # If there's a current line, save it to the list before starting a new record\n",
    "            if current_line:\n",
    "                cleaned_lines.append(current_line)\n",
    "            current_line = line  # Start a new record\n",
    "        else:\n",
    "            # This line is a continuation of the previous record\n",
    "            current_line += \" \" + line.strip()  # Merge with a space\n",
    "\n",
    "    # Add the last processed line\n",
    "    if current_line:\n",
    "        cleaned_lines.append(current_line)\n",
    "\n",
    "    return cleaned_lines\n",
    "\n",
    "# Clean and parse the RDD lines\n",
    "cleaned_rdd = data_rdd.mapPartitions(clean_and_parse)\n",
    "\n",
    "# Function to parse and fix escaping of quotes within fields\n",
    "def parse_and_escape_quotes(line):\n",
    "    # Use csv.reader to correctly parse fields with embedded quotes\n",
    "    reader = csv.reader(StringIO(line), quotechar='\"', skipinitialspace=True)\n",
    "    fields = next(reader)\n",
    "\n",
    "    # Escape internal quotes by doubling them for fields that contain quotes\n",
    "    #fields = [field.replace('\"', '\"\"') if '\"' in field else field for field in fields]\n",
    "\n",
    "    # Ensure the row has exactly 24 fields\n",
    "    if len(fields) < 24:\n",
    "        # Pad the missing fields with None\n",
    "        fields.extend([None] * (24 - len(fields)))\n",
    "    elif len(fields) > 24:\n",
    "        # Combine extra fields into the last field\n",
    "        fields = fields[:23] + [\",\".join(fields[23:])]\n",
    "\n",
    "    return Row(*fields)\n",
    "\n",
    "# Parse each cleaned line into a Row object with properly escaped quotes\n",
    "parsed_rdd = cleaned_rdd.map(parse_and_escape_quotes)\n",
    "\n",
    "# Create DataFrame from parsed RDD\n",
    "columns = header.split(\",\")  # Split the header to get column names\n",
    "anime_df = spark.createDataFrame(parsed_rdd, schema=columns)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "anime_df.show(30, truncate=False)\n",
    "\n",
    "# Define the output path\n",
    "output_path = \"project/anime_dataset_cleaned.csv\"\n",
    "\n",
    "# Save the cleaned DataFrame as a CSV file\n",
    "anime_df.write.csv(output_path, header=True, mode='overwrite', escape='\"')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing and Filtering Dataset Section\n",
    "\n",
    "This section focuses on processing a cleaned anime dataset, filtering out invalid records, and restructuring it for further analysis. The key steps performed are as follows:\n",
    "\n",
    "1. **Data Loading**:\n",
    "   The cleaned dataset is loaded as an RDD (Resilient Distributed Dataset) from a CSV file. The header row is identified and excluded from the main data for processing.\n",
    "\n",
    "2. **Line Processing**:\n",
    "   Each line in the dataset is processed to:\n",
    "   - Remove null characters to prevent parsing errors.\n",
    "   - Parse the fields using a CSV reader to handle embedded quotes and other formatting issues.\n",
    "   - Log problematic lines for debugging while ensuring data consistency.\n",
    "\n",
    "3. **Filtering Invalid Rows**:\n",
    "   Rows with specific fields marked as \"UNKNOWN\" are filtered out. This ensures that only valid records with meaningful data are retained for downstream analysis.\n",
    "\n",
    "4. **Field Selection and Transformation**:\n",
    "   Selected fields from each record are extracted and transformed:\n",
    "   - Commas in text fields like `name`, `genre`, and `studios` are replaced with semicolons to avoid issues during CSV serialization.\n",
    "   - Key features such as `anime_id`, `score`, `rank`, and `popularity` are preserved for further analysis.\n",
    "\n",
    "5. **Verification**:\n",
    "   A sample of 10 processed rows is taken and printed to validate the correctness of the processing steps. This provides a quick check to ensure the transformations are applied as intended.\n",
    "\n",
    "6. **DataFrame Creation**:\n",
    "   The processed RDD is converted into a Spark DataFrame with a predefined schema, making the data structured and suitable for analysis or storage.\n",
    "\n",
    "7. **Saving the Processed Dataset**:\n",
    "   The resulting DataFrame is saved as a new CSV file. The file includes headers, and any internal quotes in the fields are properly escaped. The overwrite mode ensures that any existing file at the output path is replaced with the new dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the file path\n",
    "filepath = \"project/anime_dataset_cleaned.csv\"\n",
    "\n",
    "# Read the CSV file as an RDD of strings\n",
    "rdd = spark.sparkContext.textFile(filepath)\n",
    "\n",
    "# Extract the header\n",
    "header = rdd.first()\n",
    "\n",
    "def process_line(line):\n",
    "    try:\n",
    "        # Remove null characters\n",
    "        line = line.replace('\\x00', '')\n",
    "        # Use csv.reader to correctly parse the quoted fields\n",
    "        reader = csv.reader(StringIO(line), quotechar='\"', skipinitialspace=True)\n",
    "        fields = next(reader)\n",
    "\n",
    "        #fields = [field.replace(r'\\\"', '\"') for field in fields]\n",
    "        return fields\n",
    "    except Exception as e:\n",
    "        # Print or log the problematic line for debugging\n",
    "        print(f\"Error processing line: {line} | Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "# Filter out the header and process the lines\n",
    "processed_rdd = rdd.filter(lambda line: line != header) \\\n",
    "    .map(process_line) \\\n",
    "    .filter(lambda row: row[4] != \"UNKNOWN\" and row[18] != \"UNKNOWN\" and row[19] != \"UNKNOWN\" and row[5] != \"UNKNOWN\" and row[14] != \"UNKNOWN\" ) \\\n",
    "    .map(lambda row: (\n",
    "        row[0],                     # anime_id\n",
    "        row[1].replace(',', ';'),   # name\n",
    "        row[4],                     # score\n",
    "        row[5].replace(',', ';'),   # genre \n",
    "        row[7],                     # type\n",
    "        row[8],                     # episodes\n",
    "        row[14].replace(',', ';'),  # studios\n",
    "        row[18],                    # rank\n",
    "        row[19],                    # popularity\n",
    "        row[20],                    # favorites\n",
    "        row[21],                    # scored_by\n",
    "        row[22]                     # members\n",
    "    ))\n",
    "\n",
    "# Take a sample of processed rows to verify correctness\n",
    "first_10_lines = processed_rdd.take(10)\n",
    "for line in first_10_lines:\n",
    "    print(line)\n",
    "\n",
    "# Define schema for the selected columns\n",
    "schema = [\"anime_id\", \"name\", \"score\", \"genre\", \"type\", \"episodes\", \"studios\", \"rank\", \"popularity\", \"favorites\", \"scored_by\", \"members\"]\n",
    "\n",
    "# Create a DataFrame from the processed RDD\n",
    "anime_df = spark.createDataFrame(processed_rdd, schema=schema)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "anime_df.show(truncate=False)\n",
    "\n",
    "# Define the output path\n",
    "output_path = \"project/anime_dataset_trimmed.csv\"\n",
    "\n",
    "# Save the DataFrame as a CSV file with overwrite mode\n",
    "anime_df.write.csv(output_path, header=True, mode='overwrite', escape='')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering for Genres and Studios\n",
    "\n",
    "This section focuses on transforming the trimmed anime dataset into a format suitable for machine learning by engineering features based on genres, studios, and other relevant attributes. Below is a summary of the steps:\n",
    "\n",
    "1. **Data Loading**:\n",
    "   The trimmed dataset is loaded as an RDD, and the header row is identified and excluded. This ensures that only valid data records are processed.\n",
    "\n",
    "2. **Computing Average Scores for Genres and Studios**:\n",
    "   - Genres and studios are extracted from each record as key-value pairs, where the key is the name of the genre or studio, and the value is a tuple of the anime's score and a count of 1.\n",
    "   - These pairs are aggregated to calculate the total score and occurrence count for each genre and studio.\n",
    "   - The average score is then computed by dividing the total score by the count. The results are stored in a dictionary for efficient lookup during feature engineering.\n",
    "\n",
    "3. **Feature Transformation**:\n",
    "   - Each line in the dataset is processed to extract relevant fields such as `rank`, `popularity`, `favorites`, and `members`.\n",
    "   - Rank and popularity are transformed into scaled features using predefined maximum values.\n",
    "   - Average scores for genres and studios are retrieved from the dictionary and combined if multiple genres or studios are associated with an anime.\n",
    "\n",
    "4. **Creating Labeled Features**:\n",
    "   - A labeled feature vector is created for each anime record, containing transformed features and a label (the anime's score).\n",
    "   - This ensures the data is structured for use in machine learning models.\n",
    "\n",
    "5. **Verification**:\n",
    "   - A sample of 10 processed rows is printed to validate that the feature engineering process is functioning as intended.\n",
    "\n",
    "6. **DataFrame Creation**:\n",
    "   - The processed RDD is converted into a Spark DataFrame, where each row contains a dense vector of features and the corresponding label.\n",
    "   - This structured format is ideal for training machine learning models in PySpark or exporting for use in other frameworks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "from io import StringIO\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "# Load CSV file as RDD\n",
    "filepath = \"project/anime_dataset_trimmed.csv\"\n",
    "rdd = spark.sparkContext.textFile(filepath)\n",
    "\n",
    "# Extract the header\n",
    "header = rdd.first()\n",
    "\n",
    "# Remove the header\n",
    "data_rdd = rdd.filter(lambda line: line != header)\n",
    "\n",
    "# Define the maximum values for rank and popularity\n",
    "MAX_RANK = 12701.0\n",
    "MAX_POPULARITY = 19191.0\n",
    "\n",
    "# Function to extract key-value pairs of genres and studios with their scores\n",
    "def extract_studio_and_genre_scores(line):\n",
    "    line = line.replace('\\x00', '')  # Clean null bytes\n",
    "    reader = csv.reader(StringIO(line), quotechar='\"', skipinitialspace=True)\n",
    "    fields = next(reader)\n",
    "\n",
    "    score = float(fields[2])  # Anime score\n",
    "    studios = fields[6].split(\";\")  # Studios field\n",
    "    genres = fields[3].split(\";\")  # Genres field\n",
    "\n",
    "    # Create key-value pairs for both studios and genres\n",
    "    return [(studio.strip(), (score, 1)) for studio in studios] + [(genre.strip(), (score, 1)) for genre in genres]\n",
    "\n",
    "\n",
    "# 1. Extract key-value pairs of genres and studios with scores using flatMap\n",
    "# 2. Aggregate scores and counts for each key (studio/genre) using reduceByKey\n",
    "# 3. Calculate the average score for each studio and genre using mapValues\n",
    "# 4. Collect the results into a dictionary for efficient lookup\n",
    "avg_scores = data_rdd.flatMap(extract_studio_and_genre_scores) \\\n",
    "    .reduceByKey(lambda acc, value: (acc[0] + value[0], acc[1] + value[1])) \\\n",
    "    .mapValues(lambda total_and_count: total_and_count[0] / total_and_count[1]) \\\n",
    "    .collectAsMap()\n",
    "\n",
    "\n",
    "# Sample output for verification\n",
    "for key, value in list(avg_scores.items())[:10]:\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Function to parse and clean the CSV line, then compute features\n",
    "def parse_line(line, avg_scores):\n",
    "    line = line.replace('\\x00', '')  # Remove null bytes\n",
    "    reader = csv.reader(StringIO(line), quotechar='\"', skipinitialspace=True)\n",
    "    fields = next(reader)\n",
    "        \n",
    "    # Extract relevant fields\n",
    "    anime_id = fields[0]\n",
    "    rank = float(fields[7])\n",
    "    popularity = float(fields[8])\n",
    "    favorites = float(fields[9])\n",
    "    scoredby = float(fields[10])\n",
    "    members = float(fields[11])\n",
    "\n",
    "    # Transform rank and popularity\n",
    "    transform_rank = MAX_RANK - rank + 1\n",
    "    transform_popularity = MAX_POPULARITY - popularity + 1\n",
    "\n",
    "    # Compute average scores for studios and genres\n",
    "    studios = fields[6].split(\";\")\n",
    "    genres = fields[3].split(\";\")\n",
    "    studio_avg_score = sum(avg_scores.get(studio.strip(), 0) for studio in studios) / len(studios)\n",
    "    genre_avg_score = sum(avg_scores.get(genre.strip(), 0) for genre in genres) / len(genres)\n",
    "\n",
    "    # Label and features\n",
    "    label = float(fields[2])  # Anime score\n",
    "    features = [\n",
    "        transform_rank,\n",
    "        transform_popularity,\n",
    "        favorites,\n",
    "        scoredby,\n",
    "        members,\n",
    "        studio_avg_score,\n",
    "        genre_avg_score,\n",
    "    ]\n",
    "\n",
    "    return LabeledPoint(label, features)\n",
    "\n",
    "# Parse dataset and apply transformations\n",
    "parsed_rdd = data_rdd.map(lambda line: parse_line(line, avg_scores)).filter(lambda row: row is not None)\n",
    "\n",
    "# Sample parsed rows for verification\n",
    "for line in parsed_rdd.take(10):\n",
    "    print(line)\n",
    "\n",
    "# Create a DataFrame with labeled features\n",
    "labeled_df = spark.createDataFrame(parsed_rdd.map(lambda lp: Row(features=Vectors.dense(lp.features), label=lp.label)))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Debugging: print the average scores for studios and genres\n",
    "print(\"Average scores for studios and genres:\")\n",
    "for key, avg_score in avg_scores.items():\n",
    "    print(f\"{key}: {avg_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Model Training and Evaluation\n",
    "\n",
    "This section focuses on training a linear regression model using the processed dataset, evaluating its predictions, and visualizing the results. Below are the key steps:\n",
    "\n",
    "1. **Train-Test Split**:\n",
    "   The dataset is randomly split into training and testing sets, with 80% of the data used for training the model and 20% reserved for evaluation. This ensures the model is trained and tested on separate subsets of the data.\n",
    "\n",
    "2. **Model Training**:\n",
    "   A linear regression model is initialized with a regularization parameter (`regParam`) to control overfitting. The model is then trained on the training data, learning the relationship between the features and the label (anime scores).\n",
    "\n",
    "3. **Making Predictions**:\n",
    "   The trained model is used to make predictions on the test data. The resulting predictions include both the actual labels and the predicted values, allowing for evaluation of the model's performance.\n",
    "\n",
    "4. **Data Collection for Visualization**:\n",
    "   The predicted and actual values are collected from the test data and split into two lists for visualization purposes. This step prepares the data for plotting.\n",
    "\n",
    "5. **Scatter Plot of Predictions**:\n",
    "   A scatter plot is created to visualize the relationship between the actual and predicted values:\n",
    "   - Each point represents a pair of actual and predicted scores.\n",
    "   - A reference line is added to indicate perfect prediction, providing a visual benchmark for model accuracy.\n",
    "\n",
    "6. **Plot Customization**:\n",
    "   The plot is enhanced with titles, axis labels, a legend, and a grid to make it informative and easy to interpret.\n",
    "\n",
    "7. **Stopping the Spark Session**:\n",
    "   The Spark session is stopped to release resources after completing the processing and analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data, test_data = labeled_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\", regParam=0.1)\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# If you want to print them out manually:\n",
    "# for row in predictions.select(\"prediction\", \"label\").collect():\n",
    "#     print(f\"Predicted: {row['prediction']:.2f}, Actual: {row['label']:.2f}\")\n",
    "\n",
    "predicted_actual = predictions.select(\"prediction\", \"label\").collect()\n",
    "\n",
    "# Split the collected data into two lists for plotting\n",
    "actual = [row[\"label\"] for row in predicted_actual]\n",
    "predicted = [row[\"prediction\"] for row in predicted_actual]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(actual, predicted, alpha=0.6, edgecolor='k', label='Predicted vs Actual')\n",
    "\n",
    "# Add a reference line for perfect prediction\n",
    "plt.plot([min(actual), max(actual)], [min(actual), max(actual)], color='red', linestyle='--', label='Perfect Prediction')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Prediction vs Actual Values\")\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
