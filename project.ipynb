{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+\n",
      "|user_id|anime_id|rating|\n",
      "+-------+--------+------+\n",
      "|1      |21      |9     |\n",
      "|1      |48      |7     |\n",
      "|1      |320     |5     |\n",
      "|1      |49      |8     |\n",
      "|1      |304     |8     |\n",
      "|1      |306     |8     |\n",
      "|1      |53      |7     |\n",
      "|1      |47      |5     |\n",
      "|1      |591     |6     |\n",
      "|1      |54      |7     |\n",
      "|1      |55      |5     |\n",
      "|1      |56      |6     |\n",
      "|1      |57      |9     |\n",
      "|1      |368     |5     |\n",
      "|1      |68      |7     |\n",
      "|1      |889     |9     |\n",
      "|1      |1519    |7     |\n",
      "|1      |58      |8     |\n",
      "|1      |1222    |7     |\n",
      "|1      |458     |4     |\n",
      "+-------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Userscore Dataset Cleaning\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the file path\n",
    "filepath = \"project/users-score-2023.csv\"\n",
    "\n",
    "# Read the CSV file as an RDD of strings\n",
    "rdd = spark.sparkContext.textFile(filepath)\n",
    "\n",
    "header = rdd.first()\n",
    "\n",
    "\n",
    "# Filter out the header and map the columns\n",
    "selected_columns_rdd = rdd.filter(lambda line: line != header) \\\n",
    "    .map(lambda line: line.split(',')) \\\n",
    "    .map(lambda row: (row[0], row[2], row[4]))\n",
    "\n",
    "schema = [\"user_id\",\"anime_id\",\"rating\"]\n",
    "\n",
    "# Create a DataFrame from the filled RDD\n",
    "users_df = spark.createDataFrame(selected_columns_rdd, schema=schema)\n",
    "users_df.show(truncate= False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"project/users-score-2023-trimmed\"\n",
    "\n",
    "# Save the DataFrame as a CSV file with overwrite mode\n",
    "users_df.write.mode(\"overwrite\").csv(output_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anime_id,score,genre,type,episodes,studios,rank,popularity,favorites,scored_by,members\n",
      "1,8.75,Action; Award Winning; Sci-Fi,TV,26.0,Sunrise,41.0,43,78525,914193.0,1771505\n",
      "5,8.38,Action; Sci-Fi,Movie,1.0,Bones,189.0,602,1448,206248.0,360978\n",
      "6,8.22,Action; Adventure; Sci-Fi,TV,26.0,Madhouse,328.0,246,15035,356739.0,727252\n",
      "7,7.25,Action; Drama; Mystery; Supernatural,TV,26.0,Sunrise,2764.0,1795,613,42829.0,111931\n",
      "8,6.94,Adventure; Fantasy; Supernatural,TV,52.0,Toei Animation,4240.0,5126,14,6413.0,15001\n",
      "15,7.92,Sports,TV,145.0,Gallop,688.0,1252,1997,86524.0,177688\n",
      "16,8.0,Comedy; Drama; Romance,TV,24.0,J.C.Staff,589.0,862,4136,81747.0,260166\n",
      "17,7.55,Comedy; Slice of Life; Sports,TV,52.0,Nippon Animation,1551.0,4212,237,12960.0,24172\n",
      "18,8.16,Action; Drama,TV,24.0,A.C.G.T.,393.0,1273,1237,97878.0,173710\n",
      "19,8.87,Drama; Mystery; Suspense,TV,74.0,Madhouse,26.0,142,47235,368569.0,1013100\n",
      "20,7.99,Action; Adventure; Fantasy,TV,220.0,Pierrot,599.0,8,76343,1883772.0,2717330\n",
      "21,8.69,Action; Adventure; Fantasy,TV,UNKNOWN,Toei Animation,55.0,20,198986,1226493.0,2168904\n",
      "22,7.86,Sports,TV,178.0,Trans Arts,805.0,1246,3004,81992.0,178273\n",
      "23,6.39,Action; Sports,TV,12.0,Toei Animation,6943.0,8288,6,1378.0,4581\n",
      "24,7.89,Comedy; Romance,TV,26.0,Studio Comet,756.0,687,5139,146597.0,320203\n",
      "25,7.38,Action; Adventure; Comedy; Sci-Fi; Ecchi,TV,24.0,Gonzo,2167.0,1546,822,53709.0,134894\n",
      "26,7.76,Action; Drama; Sci-Fi,TV,22.0,Madhouse,990.0,935,4786,54857.0,239423\n",
      "27,7.29,Action; Supernatural,TV,24.0,Gonzo,2591.0,1233,1298,79130.0,179583\n",
      "28,7.91,Comedy; Gourmet,TV,69.0,Sunrise,704.0,1992,897,40492.0,95380\n",
      "29,7.48,Action; Drama; Sci-Fi,TV,26.0,Studio Deen,1778.0,4430,180,7273.0,21391\n",
      "30,8.35,Action; Avant Garde; Award Winning; Drama; Sci-Fi; Suspense,TV,26.0,Gainax; Tatsunoko Production,204.0,45,100638,1024927.0,1718019\n",
      "31,7.46,Drama; Sci-Fi,Movie,1.0,Gainax; Production I.G,1855.0,889,687,140233.0,251617\n",
      "32,8.55,Avant Garde; Drama; Sci-Fi,Movie,1.0,Gainax; Production I.G,98.0,177,27272,586441.0,879361\n",
      "33,8.56,Action; Adventure; Drama; Fantasy; Horror,TV,25.0,OLM,96.0,325,25404,325204.0,608800\n",
      "43,8.27,Action; Award Winning; Mystery; Sci-Fi; Suspense,Movie,1.0,Production I.G,276.0,329,12339,315593.0,604276\n",
      "44,8.71,Action; Drama; Romance,OVA,4.0,Studio Deen,48.0,834,6137,134133.0,268621\n",
      "45,8.29,Action; Adventure; Comedy; Romance,TV,94.0,Gallop; Studio Deen,249.0,432,9907,230278.0,477389\n",
      "46,7.55,Drama,Movie,1.0,Gallop,1567.0,2885,88,26859.0,50732\n",
      "47,8.16,Action; Adventure; Horror; Sci-Fi; Supernatural,Movie,1.0,Tokyo Movie Shinsha,389.0,208,12961,485580.0,804581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Userscore Dataset Cleaning\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "filepath = \"project/anime_dataset_trimmed.csv\"\n",
    "rdd = spark.sparkContext.textFile(filepath)\n",
    "\n",
    "for line in rdd.take(30):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8.75,[41.0,43.0,78525.0,914193.0,1771505.0])\n",
      "(8.38,[189.0,602.0,1448.0,206248.0,360978.0])\n",
      "(8.22,[328.0,246.0,15035.0,356739.0,727252.0])\n",
      "(7.25,[2764.0,1795.0,613.0,42829.0,111931.0])\n",
      "(6.94,[4240.0,5126.0,14.0,6413.0,15001.0])\n",
      "(7.92,[688.0,1252.0,1997.0,86524.0,177688.0])\n",
      "(8.0,[589.0,862.0,4136.0,81747.0,260166.0])\n",
      "(7.55,[1551.0,4212.0,237.0,12960.0,24172.0])\n",
      "(8.16,[393.0,1273.0,1237.0,97878.0,173710.0])\n",
      "(8.87,[26.0,142.0,47235.0,368569.0,1013100.0])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLlib with RDDs\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load CSV file as RDD\n",
    "filepath = \"project/anime_dataset_trimmed.csv\"\n",
    "rdd = spark.sparkContext.textFile(filepath)\n",
    "\n",
    "\n",
    "# Extract the header\n",
    "header = rdd.first()\n",
    "\n",
    "# remove unknown and empty space \"\"\n",
    "filtered_rdd = rdd.filter(lambda line: 'UNKNOWN' not in line and '\"\"' not in line) \n",
    "\n",
    "data_rdd = filtered_rdd.filter(lambda line: line != header)\n",
    "\n",
    "def parse_line(line):\n",
    "    fields = line.split(',')\n",
    "    \n",
    "    label = float(fields[1])  # score\n",
    "    features = [\n",
    "        #fields[4],   # episodes\n",
    "        float(fields[6]),   # rank\n",
    "        float(fields[7]),   # popularity\n",
    "        float(fields[8]),   # favorites\n",
    "        float(fields[9]),  # scored_by\n",
    "        float(fields[10])   # members\n",
    "    ]\n",
    "    #return (label, features)\n",
    "    return LabeledPoint(label, Vectors.dense(features))\n",
    "\n",
    "labeled_rdd = data_rdd.map(parse_line)\n",
    "\n",
    "for line in labeled_rdd.take(10):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_rdd, test_rdd = labeled_rdd.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the Linear Regression model using Stochastic Gradient Descent (SGD)\n",
    "model = LinearRegressionWithSGD.train(train_rdd, iterations=10, step=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 2.825127872611107e+76\n",
      "Mean Absolute Error (MAE): 9.840332169946151e+75\n",
      "R-squared: -1.0825370967508432e+153\n",
      "Predicted: -1.009508824372925e+77, Actual: 8.87\n",
      "Predicted: -1.8588729614212607e+76, Actual: 7.86\n",
      "Predicted: -2.2278001046772726e+76, Actual: 7.76\n",
      "Predicted: -1.9068245315761994e+77, Actual: 8.35\n",
      "Predicted: -1.0056574854483762e+77, Actual: 8.55\n",
      "Predicted: -2.8526894436305605e+76, Actual: 8.71\n",
      "Predicted: -8.951238885727611e+76, Actual: 8.16\n",
      "Predicted: -5.593975568605258e+75, Actual: 7.29\n",
      "Predicted: -2.099670804757293e+76, Actual: 7.41\n",
      "Predicted: -3.202981859509633e+76, Actual: 8.03\n"
     ]
    }
   ],
   "source": [
    "predictions_and_labels = test_rdd.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n",
    "\n",
    "# Evaluate the model using regression metrics\n",
    "metrics = RegressionMetrics(predictions_and_labels)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Root Mean Squared Error (RMSE): {metrics.rootMeanSquaredError}\")\n",
    "print(f\"Mean Absolute Error (MAE): {metrics.meanAbsoluteError}\")\n",
    "print(f\"R-squared: {metrics.r2}\")\n",
    "\n",
    "\n",
    "for prediction, label in predictions_and_labels.take(10):\n",
    "    print(f\"Predicted: {prediction}, Actual: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver Python executable: Default Python\n",
      "Executor Python executable: Default Python\n",
      "Worker Python executable: ['/usr/bin/python3']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Print Python environment on the driver\n",
    "print(f\"Driver Python executable: {os.environ.get('PYSPARK_DRIVER_PYTHON', 'Default Python')}\")\n",
    "print(f\"Executor Python executable: {os.environ.get('PYSPARK_PYTHON', 'Default Python')}\")\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Check Python Path\").getOrCreate()\n",
    "\n",
    "# Print the Python environment used on workers\n",
    "rdd = spark.sparkContext.parallelize([1])\n",
    "def print_worker_python(_):\n",
    "    import sys\n",
    "    return sys.executable\n",
    "\n",
    "worker_python_path = rdd.map(print_worker_python).collect()\n",
    "print(f\"Worker Python executable: {worker_python_path}\")\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dat535project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
