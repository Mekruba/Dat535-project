{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heavy Load Benchmark: User Score Analysis and Average Computation\n",
    "\n",
    "This notebook focuses on performing a heavy load benchmark to evaluate the performance of PySpark in processing and analyzing large datasets. The primary goal is to handle a dataset of user scores efficiently, compute meaningful statistics, and prepare insights for downstream tasks. \n",
    "\n",
    "The workflow is divided into two key stages:\n",
    "\n",
    "1. **Data Preprocessing and Cleaning**:\n",
    "   - The raw dataset containing user scores is preprocessed to extract relevant columns, clean the data, and structure it into a trimmed and manageable format.\n",
    "   - The dataset is saved as a new file to serve as the basis for further analysis.\n",
    "\n",
    "2. **Average Score Computation**:\n",
    "   - The preprocessed dataset is analyzed to calculate the average score for each anime based on user ratings.\n",
    "   - PySparkâ€™s distributed computation capabilities are leveraged to process the data efficiently, demonstrating its scalability under heavy computational loads.\n",
    "\n",
    "This benchmark evaluates the Spark session's ability to manage memory, allocate resources across nodes, and execute parallel computations, providing insights into the framework's performance under heavy workloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing for User Scores\n",
    "\n",
    "This section focuses on loading and preprocessing a large dataset of user scores, preparing it for further analysis. Below is a summary of the steps:\n",
    "\n",
    "1. **Spark Session Initialization**:\n",
    "   - A Spark session is configured with specific parameters for distributed data processing, including memory allocation, the number of cores, and the number of executor instances.\n",
    "   - This configuration ensures the Spark environment is optimized for handling heavy workloads efficiently.\n",
    "\n",
    "2. **Data Loading**:\n",
    "   - The dataset is loaded as an RDD from a CSV file. Each line of the file is treated as a string, allowing flexible processing.\n",
    "\n",
    "3. **Header Extraction and Filtering**:\n",
    "   - The header row is extracted to identify column names.\n",
    "   - The header is then filtered out from the dataset to avoid processing it as part of the data.\n",
    "\n",
    "4. **Column Selection**:\n",
    "   - Each line in the dataset is parsed into fields using a CSV reader.\n",
    "   - Selected fields (`user_id`, `anime_id`, and `rating`) are extracted to focus on the relevant data for analysis.\n",
    "\n",
    "5. **DataFrame Creation**:\n",
    "   - The processed RDD is converted into a Spark DataFrame, with a schema (`user_id`, `anime_id`, `rating`) assigned to define the column structure.\n",
    "   - This structured format facilitates data manipulation and storage in subsequent steps.\n",
    "\n",
    "6. **Verification**:\n",
    "   - The resulting DataFrame is displayed to verify the correctness of the preprocessing and ensure the selected columns contain valid data.\n",
    "\n",
    "7. **Data Export**:\n",
    "   - The cleaned and structured DataFrame is saved as a new CSV file. The output file includes headers, and the `overwrite` mode ensures that any existing file at the specified path is replaced.\n",
    "\n",
    "This section prepares a trimmed and structured version of the user scores dataset, making it ready for downstream tasks such as data analysis or feature engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+\n",
      "|user_id|anime_id|rating|\n",
      "+-------+--------+------+\n",
      "|1      |21      |9     |\n",
      "|1      |48      |7     |\n",
      "|1      |320     |5     |\n",
      "|1      |49      |8     |\n",
      "|1      |304     |8     |\n",
      "|1      |306     |8     |\n",
      "|1      |53      |7     |\n",
      "|1      |47      |5     |\n",
      "|1      |591     |6     |\n",
      "|1      |54      |7     |\n",
      "|1      |55      |5     |\n",
      "|1      |56      |6     |\n",
      "|1      |57      |9     |\n",
      "|1      |368     |5     |\n",
      "|1      |68      |7     |\n",
      "|1      |889     |9     |\n",
      "|1      |1519    |7     |\n",
      "|1      |58      |8     |\n",
      "|1      |1222    |7     |\n",
      "|1      |458     |4     |\n",
      "+-------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Variables for Spark configuration\n",
    "spark_master = \"yarn\"\n",
    "driver_memory = \"2048m\"\n",
    "am_memory = \"1024m\"\n",
    "executor_memory = \"2g\"\n",
    "executor_cores = \"2\"\n",
    "executor_instances = \"6\"\n",
    "max_cores = \"12\"\n",
    "\n",
    "# Create a Spark session with the configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Benchmark heavy load 3 config 5\") \\\n",
    "    .master(spark_master) \\\n",
    "    .config(\"spark.driver.memory\", driver_memory) \\\n",
    "    .config(\"spark.yarn.am.memory\", am_memory) \\\n",
    "    .config(\"spark.executor.memory\", executor_memory) \\\n",
    "    .config(\"spark.executor.cores\", executor_cores) \\\n",
    "    .config(\"spark.executor.instances\", executor_instances) \\\n",
    "    .config(\"spark.cores.max\", max_cores) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Define the file path\n",
    "filepath = \"project/users-score-2023.csv\"\n",
    "\n",
    "# Read the CSV file as an RDD of strings\n",
    "rdd = spark.sparkContext.textFile(filepath)\n",
    "\n",
    "def process_line(line):\n",
    "    reader = csv.reader(StringIO(line), quotechar='\"', skipinitialspace=True)\n",
    "    fields = next(reader)\n",
    "\n",
    "    return fields\n",
    "\n",
    "header = rdd.first()\n",
    "\n",
    "\n",
    "# Filter out the header and map the columns\n",
    "selected_columns_rdd = rdd.filter(lambda line: line != header) \\\n",
    "    .map(process_line) \\\n",
    "    .map(lambda row: (row[0], row[2], row[4]))\n",
    "\n",
    "schema = [\"user_id\",\"anime_id\",\"rating\"]\n",
    "\n",
    "# Create a DataFrame from the filled RDD\n",
    "users_df = spark.createDataFrame(selected_columns_rdd, schema=schema)\n",
    "users_df.show(truncate= False)\n",
    "\n",
    "\n",
    "output_path = \"project/users-score-2023-trimmed\"\n",
    "\n",
    "# Save the DataFrame as a CSV file with overwrite mode\n",
    "users_df.write.mode(\"overwrite\").csv(output_path, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Score Computation for Anime\n",
    "\n",
    "This section focuses on calculating the average score for each anime based on user ratings in a preprocessed dataset. The steps are as follows:\n",
    "\n",
    "1. **Data Loading**:\n",
    "   - The trimmed dataset containing user scores is loaded as an RDD from the specified file path.\n",
    "   - Each line in the file is read as a string, and the header row is identified for exclusion.\n",
    "\n",
    "2. **Data Parsing and Filtering**:\n",
    "   - The header is filtered out to ensure only valid data records are processed.\n",
    "   - Each line is split into fields, and the relevant columns (`anime_id` and `rating`) are extracted. Ratings are converted into floating-point numbers for numerical computations.\n",
    "\n",
    "3. **Score Aggregation**:\n",
    "   - Each record is mapped into a key-value pair where the key is the `anime_id` and the value is a tuple containing the score and a count of 1.\n",
    "   - The `reduceByKey` function is used to aggregate the scores and counts for each `anime_id`. This step calculates the total score and the number of ratings for each anime.\n",
    "\n",
    "4. **Average Score Calculation**:\n",
    "   - After aggregation, the average score for each anime is computed by dividing the total score by the count of ratings. This step results in an RDD containing `anime_id` as the key and the computed average score as the value.\n",
    "\n",
    "5. **Results Collection**:\n",
    "   - A subset of the results (e.g., 50 records) is collected and printed to verify the correctness of the computations.\n",
    "   - Each result displays the `anime_id` and its corresponding average score, formatted to two decimal places.\n",
    "\n",
    "6. **Stopping the Spark Session**:\n",
    "   - The Spark session is stopped after the computation is complete to release resources.\n",
    "\n",
    "This section provides a scalable and efficient way to compute average scores for anime titles based on user ratings, leveraging PySpark's distributed processing capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anime ID: 11813, Average Score: 7.74\n",
      "Anime ID: 16742, Average Score: 7.23\n",
      "Anime ID: 22537, Average Score: 6.45\n",
      "Anime ID: 53, Average Score: 7.23\n",
      "Anime ID: 16201, Average Score: 6.99\n",
      "Anime ID: 54, Average Score: 7.21\n",
      "Anime ID: 28249, Average Score: 7.69\n",
      "Anime ID: 2251, Average Score: 8.56\n",
      "Anime ID: 7674, Average Score: 8.25\n",
      "Anime ID: 12365, Average Score: 8.62\n",
      "Anime ID: 13535, Average Score: 7.82\n",
      "Anime ID: 740, Average Score: 7.63\n",
      "Anime ID: 1239, Average Score: 7.53\n",
      "Anime ID: 1278, Average Score: 7.30\n",
      "Anime ID: 3076, Average Score: 7.38\n",
      "Anime ID: 1498, Average Score: 7.28\n",
      "Anime ID: 889, Average Score: 8.22\n",
      "Anime ID: 3389, Average Score: 6.66\n",
      "Anime ID: 2341, Average Score: 6.37\n",
      "Anime ID: 1474, Average Score: 7.52\n",
      "Anime ID: 1476, Average Score: 7.61\n",
      "Anime ID: 225, Average Score: 6.62\n",
      "Anime ID: 3784, Average Score: 8.65\n",
      "Anime ID: 504, Average Score: 6.77\n",
      "Anime ID: 1611, Average Score: 5.96\n",
      "Anime ID: 27631, Average Score: 7.17\n",
      "Anime ID: 16099, Average Score: 6.18\n",
      "Anime ID: 4999, Average Score: 7.05\n",
      "Anime ID: 1689, Average Score: 8.22\n",
      "Anime ID: 5356, Average Score: 7.38\n",
      "Anime ID: 3841, Average Score: 7.76\n",
      "Anime ID: 2124, Average Score: 7.05\n",
      "Anime ID: 1194, Average Score: 6.75\n",
      "Anime ID: 1817, Average Score: 7.01\n",
      "Anime ID: 4789, Average Score: 8.22\n",
      "Anime ID: 227, Average Score: 8.03\n",
      "Anime ID: 134, Average Score: 7.57\n",
      "Anime ID: 3299, Average Score: 7.28\n",
      "Anime ID: 256, Average Score: 7.71\n",
      "Anime ID: 13331, Average Score: 7.94\n",
      "Anime ID: 389, Average Score: 7.65\n",
      "Anime ID: 33, Average Score: 8.29\n",
      "Anime ID: 218, Average Score: 7.64\n",
      "Anime ID: 6045, Average Score: 8.26\n",
      "Anime ID: 43, Average Score: 8.31\n",
      "Anime ID: 97, Average Score: 7.97\n",
      "Anime ID: 99, Average Score: 7.41\n",
      "Anime ID: 877, Average Score: 8.51\n",
      "Anime ID: 30, Average Score: 8.25\n",
      "Anime ID: 1117, Average Score: 7.33\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filepath to the trimmed dataset\n",
    "filepath = \"project/users-score-2023-trimmed\"\n",
    "\n",
    "# Read the file as an RDD\n",
    "rdd = spark.sparkContext.textFile(filepath)\n",
    "\n",
    "# Extract the header\n",
    "header = rdd.first()\n",
    "\n",
    "# Define the process_line function\n",
    "def process_line(line):\n",
    "    fields = line.split(\",\")\n",
    "    return fields\n",
    "\n",
    "# Filter out the header and parse the data\n",
    "data_rdd = rdd.filter(lambda line: line != header) \\\n",
    "              .map(process_line) \\\n",
    "              .map(lambda cols: (cols[1], float(cols[2])))  # (anime_id, score)\n",
    "\n",
    "# Compute the sum and count for each anime_id using map and reduce\n",
    "# Step 1: Map each record to a tuple (anime_id, (score, 1))\n",
    "# Step 2: Use reduce to aggregate the sum and count\n",
    "# Step 3: Calculate the average score for each anime_id\n",
    "average_rdd = data_rdd.map(lambda x: (x[0], (x[1], 1))) \\\n",
    "                      .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) \\\n",
    "                      .map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
    "\n",
    "\n",
    "# Collect and display the results\n",
    "results = average_rdd.take(50)\n",
    "for anime_id, avg_score in results:\n",
    "    print(f\"Anime ID: {anime_id}, Average Score: {avg_score:.2f}\")\n",
    "\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dat535project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
