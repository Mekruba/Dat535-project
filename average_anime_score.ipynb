{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heavy Load Benchmark: User Score Analysis and Average Computation\n",
    "\n",
    "This notebook focuses on performing a heavy load benchmark to evaluate the performance of PySpark in processing and analyzing large datasets. The primary goal is to handle a dataset of user scores efficiently, compute meaningful statistics, and prepare insights for downstream tasks. \n",
    "\n",
    "The workflow is divided into two key stages:\n",
    "\n",
    "1. **Data Preprocessing and Cleaning**:\n",
    "   - The raw dataset containing user scores is preprocessed to extract relevant columns, clean the data, and structure it into a trimmed and manageable format.\n",
    "   - The dataset is saved as a new file to serve as the basis for further analysis.\n",
    "\n",
    "2. **Average Score Computation**:\n",
    "   - The preprocessed dataset is analyzed to calculate the average score for each anime based on user ratings.\n",
    "   - PySparkâ€™s distributed computation capabilities are leveraged to process the data efficiently, demonstrating its scalability under heavy computational loads.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset # reset all stored variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing for User Scores\n",
    "\n",
    "This section focuses on loading and preprocessing a large dataset of user scores, preparing it for further analysis. Below is a summary of the steps:\n",
    "\n",
    "1. **Spark Session Initialization**:\n",
    "   - A Spark session is configured with specific parameters for distributed data processing, including memory allocation, the number of cores, and the number of executor instances.\n",
    "   - This configuration ensures the Spark environment is optimized for handling heavy workloads efficiently.\n",
    "\n",
    "2. **Data Loading**:\n",
    "   - The dataset is loaded as an RDD from a CSV file. Each line of the file is treated as a string, allowing flexible processing.\n",
    "\n",
    "3. **Header Extraction and Filtering**:\n",
    "   - The header row is extracted to identify column names.\n",
    "   - The header is then filtered out from the dataset to avoid processing it as part of the data.\n",
    "\n",
    "4. **Column Selection**:\n",
    "   - Each line in the dataset is parsed into fields using a CSV reader.\n",
    "   - Selected fields (`user_id`, `anime_id`, and `rating`) are extracted to focus on the relevant data for analysis.\n",
    "\n",
    "5. **DataFrame Creation**:\n",
    "   - The processed RDD is converted into a Spark DataFrame, with a schema (`user_id`, `anime_id`, `rating`) assigned to define the column structure.\n",
    "   - This structured format facilitates data manipulation and storage in subsequent steps.\n",
    "\n",
    "6. **Verification**:\n",
    "   - The resulting DataFrame is displayed to verify the correctness of the preprocessing and ensure the selected columns contain valid data.\n",
    "\n",
    "7. **Data Export**:\n",
    "   - The cleaned and structured DataFrame is saved as a new CSV file. The output file includes headers, and the `overwrite` mode ensures that any existing file at the specified path is replaced.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Variables for Spark configuration\n",
    "spark_master = \"yarn\"\n",
    "driver_memory = \"2048m\"\n",
    "am_memory = \"1024m\"\n",
    "executor_memory = \"2g\"\n",
    "executor_cores = \"2\"\n",
    "executor_instances = \"6\"\n",
    "max_cores = \"12\"\n",
    "\n",
    "# Create a Spark session with the configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Benchmark heavy load 3 config 5\") \\\n",
    "    .master(spark_master) \\\n",
    "    .config(\"spark.driver.memory\", driver_memory) \\\n",
    "    .config(\"spark.yarn.am.memory\", am_memory) \\\n",
    "    .config(\"spark.executor.memory\", executor_memory) \\\n",
    "    .config(\"spark.executor.cores\", executor_cores) \\\n",
    "    .config(\"spark.executor.instances\", executor_instances) \\\n",
    "    .config(\"spark.cores.max\", max_cores) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Define the file path\n",
    "filepath = \"project/users-score-2023.csv\"\n",
    "\n",
    "# Read the CSV file as an RDD of strings\n",
    "rdd = spark.sparkContext.textFile(filepath)\n",
    "\n",
    "def process_line(line):\n",
    "    reader = csv.reader(StringIO(line), quotechar='\"', skipinitialspace=True)\n",
    "    fields = next(reader)\n",
    "\n",
    "    return fields\n",
    "\n",
    "header = rdd.first()\n",
    "\n",
    "\n",
    "# Filter out the header and map the columns\n",
    "selected_columns_rdd = rdd.filter(lambda line: line != header) \\\n",
    "    .map(process_line) \\\n",
    "    .map(lambda row: (row[0], row[2], row[4]))\n",
    "\n",
    "schema = [\"user_id\",\"anime_id\",\"rating\"]\n",
    "\n",
    "# Create a DataFrame from the filled RDD\n",
    "users_df = spark.createDataFrame(selected_columns_rdd, schema=schema)\n",
    "users_df.show(truncate= False)\n",
    "\n",
    "\n",
    "output_path = \"project/users-score-2023-trimmed\"\n",
    "\n",
    "# Save the DataFrame as a CSV file with overwrite mode\n",
    "users_df.write.mode(\"overwrite\").csv(output_path, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Score Computation for Anime\n",
    "\n",
    "This section focuses on calculating the average score for each anime based on user ratings in a preprocessed dataset. The steps are as follows:\n",
    "\n",
    "1. **Data Loading**:\n",
    "   - The trimmed dataset containing user scores is loaded as an RDD from the specified file path.\n",
    "   - Each line in the file is read as a string, and the header row is identified for exclusion.\n",
    "\n",
    "2. **Data Parsing and Filtering**:\n",
    "   - The header is filtered out to ensure only valid data records are processed.\n",
    "   - Each line is split into fields, and the relevant columns (`anime_id` and `rating`) are extracted. Ratings are converted into floating-point numbers for numerical computations.\n",
    "\n",
    "3. **Score Aggregation**:\n",
    "   - Each record is mapped into a key-value pair where the key is the `anime_id` and the value is a tuple containing the score and a count of 1.\n",
    "   - The `reduceByKey` function is used to aggregate the scores and counts for each `anime_id`. This step calculates the total score and the number of ratings for each anime.\n",
    "\n",
    "4. **Average Score Calculation**:\n",
    "   - After aggregation, the average score for each anime is computed by dividing the total score by the count of ratings. This step results in an RDD containing `anime_id` as the key and the computed average score as the value.\n",
    "\n",
    "5. **Results Collection**:\n",
    "   - A subset of the results (e.g., 50 records) is collected and printed to verify the correctness of the computations.\n",
    "   - Each result displays the `anime_id` and its corresponding average score, formatted to two decimal places.\n",
    "\n",
    "6. **Stopping the Spark Session**:\n",
    "   - The Spark session is stopped after the computation is complete to release resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filepath to the trimmed dataset\n",
    "filepath = \"project/users-score-2023-trimmed\"\n",
    "\n",
    "# Read the file as an RDD\n",
    "rdd = spark.sparkContext.textFile(filepath)\n",
    "\n",
    "# Extract the header\n",
    "header = rdd.first()\n",
    "\n",
    "# Define the process_line function\n",
    "def process_line(line):\n",
    "    fields = line.split(\",\")\n",
    "    return fields\n",
    "\n",
    "# Filter out the header and parse the data\n",
    "data_rdd = rdd.filter(lambda line: line != header) \\\n",
    "              .map(process_line) \\\n",
    "              .map(lambda cols: (cols[1], float(cols[2])))  # (anime_id, score)\n",
    "\n",
    "# Compute the sum and count for each anime_id using map and reduce\n",
    "# Step 1: Map each record to a tuple (anime_id, (score, 1))\n",
    "# Step 2: Use reduce to aggregate the sum and count\n",
    "# Step 3: Calculate the average score for each anime_id\n",
    "average_rdd = data_rdd.map(lambda x: (x[0], (x[1], 1))) \\\n",
    "                      .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) \\\n",
    "                      .map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
    "\n",
    "\n",
    "# Collect and display the results\n",
    "results = average_rdd.take(50)\n",
    "for anime_id, avg_score in results:\n",
    "    print(f\"Anime ID: {anime_id}, Average Score: {avg_score:.2f}\")\n",
    "\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dat535project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
